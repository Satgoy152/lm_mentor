{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs\n",
      "Loaded subject mapping\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "\n",
    "#loading pdfs\n",
    "# LEGACY CODE\n",
    "print(\"Loading PDFs\")\n",
    "# print(\"Course description...\", end=\"\\r\", flush=True)\n",
    "# pdf_loader = PyPDFLoader(\"rag_data/Umich_FA2024_course_description.pdf\")\n",
    "# course_description_text = pdf_loader.load()\n",
    "# print(\"Loaded course description\")\n",
    "# print(\"Degree requirements...\", end=\"\\r\", flush=True)\n",
    "# pdf_loader = PyPDFLoader(\"rag_data/Umich_FA2024_LSA_degree_requ.pdf\")\n",
    "# degree_requirements_text = pdf_loader.load()\n",
    "# print(\"Loaded degree requirements\")\n",
    "# print(\"Major minor description...\", end=\"\\r\", flush=True)\n",
    "# pdf_loader = PyPDFLoader(\"rag_data/Umich_FA2024_major_minor_description.pdf\")\n",
    "# major_minor_description_text = pdf_loader.load()\n",
    "# print(\"Loaded major minor description\")\n",
    "\n",
    "# print(\"Subject mapping...\", end=\"\\r\", flush=True)\n",
    "# pdf_loader = PyPDFLoader(\"rag_data/Umich_FA2024_subject_mapping.pdf\")\n",
    "# subject_mapping_text = pdf_loader.load()\n",
    "# print(\"Loaded subject mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# load course descriptions\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"rag_data/Umich_FA2024_course_description.md\", mode=\"elements\")\n",
    "course_description_text = loader.load()\n",
    "\n",
    "for doc in course_description_text:\n",
    "    try:\n",
    "        doc.metadata = {'source': 'rag_data/Umich_FA2024_course_description.md', 'course': str(doc.metadata['emphasized_text_contents'][0])}\n",
    "    except:\n",
    "        print(doc.metadata)\n",
    "        doc.metadata = {'source': 'rag_data/Umich_FA2024_course_description.md'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5t/bmhkrbcn4yzgbdrrgwk7ps_r0000gn/T/ipykernel_4111/3119957408.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dfs = [df.applymap(lambda x: str(x) + \" | \") for df in dfs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"rag_data/Umich_FA2024_course_schedule.csv\")\n",
    "# combind Subject\tCatalog Nbr\n",
    "df[\"Class Identifier\"] = df[\"Subject\"] + \" \" + df[\"Catalog Nbr\"]\n",
    "# create a list of dfs for each unique class identifier\n",
    "dfs = [df[df[\"Class Identifier\"] == class_identifier] for class_identifier in df[\"Class Identifier\"].unique()]\n",
    "# move 'Class Identifier' column to the front of each df\n",
    "dfs = [pd.concat([df[\"Class Identifier\"], df.drop(columns=[\"Class Identifier\"])], axis=1) for df in dfs]\n",
    "# drop  Subject\tCatalog Nbr columns from each df\n",
    "dfs = [df.drop(columns=[\"Subject\", \"Catalog Nbr\"]) for df in dfs]\n",
    "# drop 'Unnamed: 23' column from each df\n",
    "dfs = [df.drop(columns=[\"Unnamed: 23\"]) for df in dfs]\n",
    "# add | between elements in each row of each df\n",
    "dfs = [df.applymap(lambda x: str(x) + \" | \") for df in dfs]\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "course_schedule_text = []\n",
    "for df in dfs:\n",
    "    doc = Document(page_content=df.to_string(index=False, header=False), metadata={\"source\": \"rag_data/Umich_FA2024_course_schedule.csv\", \"course\": df[\"Class Identifier\"].iloc[0]})\n",
    "    if len(doc.page_content) > 25000:\n",
    "        # split the document into 4 smaller documents\n",
    "        size = len(doc.page_content)\n",
    "        for i in range(8):\n",
    "            course_schedule_text.append(Document(page_content=doc.page_content[i*int(size/8):(i+1)*int(size/8)], metadata=doc.metadata))\n",
    "    elif len(doc.page_content) > 10000:\n",
    "            # split the document into 4 smaller documents\n",
    "            size = len(doc.page_content)\n",
    "            for i in range(4):\n",
    "                course_schedule_text.append(Document(page_content=doc.page_content[i*int(size/4):(i+1)*int(size/4)], metadata=doc.metadata))\n",
    "\n",
    "    elif len(doc.page_content) > 5000:\n",
    "        # split the document into 2 smaller documents\n",
    "        size = len(doc.page_content)\n",
    "        for i in range(2):\n",
    "            course_schedule_text.append(Document(page_content=doc.page_content[i*int(size/2):(i+1)*int(size/2)], metadata=doc.metadata))\n",
    "    else:\n",
    "        course_schedule_text.append(doc)\n",
    "\n",
    "\n",
    "# number of documents with over 4k characters\n",
    "course_schedule_text = [doc for doc in course_schedule_text if len(doc.page_content) < 5500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# chunking text\n",
    "print(\"Chunking text...\")\n",
    "# LEGACY CODE\n",
    "# pages = [course_description_text, degree_requirements_text, major_minor_description_text, subject_mapping_text, course_schedule_text]\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "# course_description_chunks = text_splitter.split_documents(course_description_text)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "# degree_requirements_chunks = text_splitter.split_documents(degree_requirements_text)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "# major_minor_description_chunks = text_splitter.split_documents(major_minor_description_text)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=30)\n",
    "# subject_mapping_chunks = text_splitter.split_documents(subject_mapping_text)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "# course_schedule_chunks = text_splitter.split_documents(course_schedule_text)\n",
    "\n",
    "\n",
    "# chunked_pages = []\n",
    "# for page in pages:\n",
    "#     chunks = text_splitter.split_documents(page)\n",
    "#     chunked_pages.append(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chunks = course_description_text + course_schedule_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size None\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "import dotenv\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "# cell to load database - VERY EXPENSIVE\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "new_client = chromadb.PersistentClient(path = \"./chroma_db\", tenant = DEFAULT_TENANT, database = DEFAULT_DATABASE, settings = Settings())\n",
    "\n",
    "embeddings = VoyageAIEmbeddings(\n",
    "    voyage_api_key=dotenv.get_key(dotenv_path= \".env\", key_to_get = \"VOYAGEAI_KEY\") , model=\"voyage-large-2-instruct\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents= combined_chunks , embedding=embeddings, collection_name=\"umich_fa2024\", client=new_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm_mentor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
